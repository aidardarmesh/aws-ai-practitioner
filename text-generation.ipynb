{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056e7814-32ed-4693-bd6f-3a03129e7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "from IPython.display import display, Markdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f688998-6d82-41ee-b554-a5830dd951d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock client\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f32ec94-14ed-4f25-88dc-169ee827cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model IDs that will be used in this module\n",
    "MODELS = {\n",
    "    \"Claude 3.7 Sonnet\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    \"Claude 3.5 Sonnet\": \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    \"Claude 3.5 Haiku\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    \"Amazon Nova Pro\": \"us.amazon.nova-pro-v1:0\",\n",
    "    \"Amazon Nova Micro\": \"us.amazon.nova-micro-v1:0\",\n",
    "    \"DeepSeek-R1\": \"us.deepseek.r1-v1:0\",\n",
    "    \"Meta Llama 3.1 70B Instruct\": \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d858feb-a0b2-4f99-a427-24b974e111d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to display model responses in a more readable format\n",
    "def display_response(response, model_name=None):\n",
    "    if model_name:\n",
    "        display(Markdown(f\"### Response from {model_name}\"))\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d64082f8-baba-489f-b0a3-eeeb70ebd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_summarize = \"\"\"\n",
    "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
    "a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
    "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
    "democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
    "for text and images—including Amazons Titan FMs, which consist of two new LLMs we're also announcing \\\n",
    "today—through a scalable, reliable, and secure AWS managed service. With Bedrock's serverless experience, \\\n",
    "customers can easily find the right model for what they're trying to get done, get started quickly, privately \\\n",
    "customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
    "tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
    "with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd594ba9-b434-4091-87eb-2d00abc1bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt for summarization\n",
    "prompt = f\"\"\"Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\n",
    "<text>\n",
    "{text_to_summarize}\n",
    "</text>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f680273-3311-466b-b653-0a285dd8dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create request body for Claude 3.7 Sonnet\n",
    "claude_body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "        }\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19749334-25ca-4e58-935e-d146f8282528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Invoke Model API)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Summary\n",
       "\n",
       "Amazon announced Amazon Bedrock, a new service that provides API access to foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon. Bedrock aims to democratize access to generative AI by offering an easy way for customers to build and scale applications. The service includes text and image models, including Amazon's newly announced Titan LLMs. As a serverless AWS managed service, Bedrock allows customers to find appropriate models, customize them privately with their own data, and integrate them into applications using familiar AWS tools without managing infrastructure. The service integrates with Amazon SageMaker features like Experiments and Pipelines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send request to Claude 3.7 Sonnet\n",
    "try:\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        body=claude_body,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    \n",
    "    # Extract and display the response text\n",
    "    claude_summary = response_body[\"content\"][0][\"text\"]\n",
    "    display_response(claude_summary, \"Claude 3.7 Sonnet (Invoke Model API)\")\n",
    "    \n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "            \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "            \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "            \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2257bcb-f5d1-4acc-b765-1fd6e0691f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a converse request with our summarization task\n",
    "converse_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"Please provide a concise summary of the following text in 2-3 sentences. Text to summarize: {text_to_summarize}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"temperature\": 0.4,\n",
    "        \"topP\": 0.9,\n",
    "        \"maxTokens\": 500\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d689386a-a75d-4d6d-8bd2-b709e827842f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Converse API)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service providing API access to foundation models (FMs) from various AI companies, including Amazon's own Titan LLMs. The service democratizes generative AI by offering a serverless experience where customers can easily find, customize, and deploy text and image models without managing infrastructure. Bedrock integrates with existing AWS tools and allows private customization with customers' own data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call Claude 3.7 Sonnet with Converse API\n",
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        messages=converse_request[\"messages\"],\n",
    "        inferenceConfig=converse_request[\"inferenceConfig\"]\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response\n",
    "    claude_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(claude_converse_response, \"Claude 3.7 Sonnet (Converse API)\")\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
    "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57a8c29b-f87a-4b3e-9c76-8c0f93e99672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully called Claude 3.7 Sonnet (took 3.73 seconds)\n",
      "✅ Successfully called Claude 3.5 Sonnet (took 3.12 seconds)\n",
      "✅ Successfully called Claude 3.5 Haiku (took 2.82 seconds)\n",
      "✅ Successfully called Amazon Nova Pro (took 1.08 seconds)\n",
      "✅ Successfully called Amazon Nova Micro (took 0.53 seconds)\n",
      "✅ Successfully called DeepSeek-R1 (took 1.88 seconds)\n",
      "✅ Successfully called Meta Llama 3.1 70B Instruct (took 4.2 seconds)\n"
     ]
    }
   ],
   "source": [
    "# call different models with the same converse request\n",
    "results = {}    \n",
    "for model_name, model_id in MODELS.items(): # looping over all models defined above\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = bedrock.converse(\n",
    "                modelId=model_id,\n",
    "                messages=converse_request[\"messages\"],\n",
    "                inferenceConfig=converse_request[\"inferenceConfig\"] if \"inferenceConfig\" in converse_request else None\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Extract the model's response using the correct structure\n",
    "            model_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            response_time = round(end_time - start_time, 2)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                \"response\": model_response,\n",
    "                \"time\": response_time\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ Successfully called {model_name} (took {response_time} seconds)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error calling {model_name}: {str(e)}\")\n",
    "            results[model_name] = {\n",
    "                \"response\": f\"Error: {str(e)}\",\n",
    "                \"time\": None\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d58369f1-5d94-41a2-b949-179915f79023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Claude 3.7 Sonnet (took 3.73 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service providing API access to foundation models (FMs) from various AI companies, including Amazon's own Titan LLMs. Bedrock democratizes generative AI by offering a serverless experience where customers can easily select, customize, and deploy FMs for text and image applications without managing infrastructure. The service integrates with existing AWS tools and provides a secure, scalable environment for developers to build AI applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude 3.5 Sonnet (took 3.12 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon has announced Bedrock, a new service that provides API access to various foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon itself, including two new large language models from Amazon's Titan series. Bedrock aims to democratize access to generative AI by offering a user-friendly, serverless experience that allows customers to easily find, customize, and deploy FMs for text and image applications. The service integrates with existing AWS tools and eliminates the need for infrastructure management, making it simpler for builders to scale their AI-based applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude 3.5 Haiku (took 2.82 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service that provides easy access to foundation models from various AI companies through an API, enabling customers to build and scale generative AI applications. The service offers a serverless experience with models from AI21 Labs, Anthropic, Stability AI, and Amazon, including new Titan LLMs, allowing users to privately customize models and integrate them into applications using familiar AWS tools. Bedrock aims to democratize access to powerful AI models by simplifying the process of finding, testing, and deploying foundation models without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Amazon Nova Pro (took 1.08 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service that provides easy access to Foundation Models (FMs) from various providers like AI21 Labs, Anthropic, Stability AI, and Amazon via an API, enabling customers to build and scale generative AI applications. Bedrock offers a range of powerful text and image FMs, including Amazon's new Titan FMs, through a secure, scalable, and reliable AWS managed service, allowing users to customize, integrate, and deploy models seamlessly using familiar AWS tools without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Amazon Nova Micro (took 0.53 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service that provides easy access to generative AI models from AI21 Labs, Anthropic, Stability AI, and Amazon via an API, enabling developers to quickly build and scale AI applications without managing infrastructure. Bedrock offers scalable, secure access to a range of powerful models for text and images, including Amazon's new Titan models, and integrates seamlessly with AWS tools like SageMaker."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### DeepSeek-R1 (took 1.88 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "AWS has introduced Amazon Bedrock, a service enabling developers to build and scale generative AI applications using foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon—including Amazon’s new Titan models—via a serverless API. Bedrock simplifies access to diverse FMs for text and image tasks, allowing customization with private data, seamless AWS integration, and deployment without infrastructure management. It also supports tools like Amazon SageMaker for testing and scaling models, making AI development more accessible and efficient."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Meta Llama 3.1 70B Instruct (took 4.2 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Here is a 3-sentence summary of the text:\n",
       "\n",
       "Amazon Web Services (AWS) has launched Amazon Bedrock, a new service that provides access to foundation models (FMs) from leading AI labs via an API. Bedrock makes it easy for customers to build and scale generative AI-based applications using FMs, with a serverless experience that allows for private customization and seamless integration with AWS tools. The service offers a range of powerful FMs for text and images, including Amazon's new Titan FMs, and is designed to democratize access to AI technology for all builders."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display results in a formatted way\n",
    "for model_name, result in results.items():\n",
    "    if \"Error\" not in result[\"response\"]:\n",
    "        display(Markdown(f\"### {model_name} (took {result['time']} seconds)\"))\n",
    "        display(Markdown(result[\"response\"]))\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f833c7ff-bc83-472b-8d2d-1b7be22e83f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard response: Amazon has announced Bedrock, a new service providing API access to foundation models (FMs) from various AI companies and Amazon itself. Bedrock aims to democratize access to generative AI by offering a user-friendly platform for building and scaling AI applications. The service includes Amazon's own Titan FMs and integrates with existing AWS tools, allowing customers to easily customize, deploy, and manage AI models without infrastructure concerns.\n",
      "Cross-region response: Amazon has announced Bedrock, a new service that provides API access to Foundation Models (FMs) from various AI companies, including Amazon's own Titan models. Bedrock aims to democratize access to generative AI by offering a user-friendly, serverless platform for building and scaling AI applications. The service allows customers to easily find, customize, and deploy FMs using familiar AWS tools and infrastructure, without the need to manage complex systems.\n"
     ]
    }
   ],
   "source": [
    "# Regular model invocation (standard region)\n",
    "standard_response = bedrock.converse(\n",
    "    modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Standard model ID\n",
    "    messages=converse_request[\"messages\"]\n",
    ")\n",
    "\n",
    "# Cross-region inference (note the \"us.\" prefix)\n",
    "cris_response = bedrock.converse(\n",
    "    modelId=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Cross-region model ID with regional prefix\n",
    "    messages=converse_request[\"messages\"]\n",
    ")\n",
    "\n",
    "# Print responses\n",
    "print(\"Standard response:\", standard_response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "print(\"Cross-region response:\", cris_response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9309e8b-d251-47f9-98d7-0d3512cede35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Multi-turn conversation)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock is AWS's new serverless service that provides API access to foundation models from multiple AI companies, allowing developers to easily select, customize, and deploy AI models without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of a multi-turn conversation with Converse API\n",
    "multi_turn_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"Please summarize this text: {text_to_summarize}\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": results[\"Claude 3.7 Sonnet\"][\"response\"]}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Can you make this summary even shorter, just 1 sentence?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        messages=multi_turn_messages,\n",
    "        inferenceConfig={\"temperature\": 0.2, \"maxTokens\": 500}\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response using the correct structure\n",
    "    follow_up_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(follow_up_response, \"Claude 3.7 Sonnet (Multi-turn conversation)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cc8f059-dcc7-49f9-ab7d-15de2385e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of streaming with Converse API\n",
    "def stream_converse(model_id, messages, inference_config=None):\n",
    "    if inference_config is None:\n",
    "        inference_config = {}\n",
    "    \n",
    "    print(\"Streaming response (chunks will appear as they are received):\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=inference_config\n",
    "        )\n",
    "        response_stream = response.get('stream')\n",
    "        if response_stream:\n",
    "            for event in response_stream:\n",
    "\n",
    "                if 'messageStart' in event:\n",
    "                    print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "                if 'contentBlockDelta' in event:\n",
    "                    print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "                if 'messageStop' in event:\n",
    "                    print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "                if 'metadata' in event:\n",
    "                    metadata = event['metadata']\n",
    "                    if 'usage' in metadata:\n",
    "                        print(\"\\nToken usage\")\n",
    "                        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                        print(\n",
    "                            f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                        print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                    if 'metrics' in event['metadata']:\n",
    "                        print(\n",
    "                            f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "                \n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "        return full_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in streaming: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfdc8146-089e-420b-8f98-60823b54cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try streaming a longer summary\n",
    "streaming_request = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": f\"\"\"Please provide a detailed summary of the following text, explaining its key points and implications:\n",
    "                \n",
    "                {text_to_summarize}\n",
    "                \n",
    "                Make your summary comprehensive but clear.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b64d53eb-aac0-477e-b5a0-6e59d3e08076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response (chunks will appear as they are received):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Role: assistant\n",
      "# Summary: Amazon Bedrock - Democratizing Access to Foundation Models\n",
      "\n",
      "## Key Points\n",
      "\n",
      "Amazon has launched Amazon Bedrock, a new service that provides API access to Foundation Models (FMs) from multiple AI companies including:\n",
      "- AI21 Labs\n",
      "- Anthropic\n",
      "- Stability AI\n",
      "- Amazon's own models\n",
      "\n",
      "The service is designed as a response to customer feedback, aiming to make generative AI more accessible to all developers.\n",
      "\n",
      "## Core Features\n",
      "\n",
      "1. **Diverse Model Access**: Offers a range of powerful FMs for both text and image generation\n",
      "\n",
      "2. **Amazon Titan Models**: Introduces two new Large Language Models (LLMs) developed by Amazon\n",
      "\n",
      "3. **Serverless Architecture**: Eliminates infrastructure management needs for users\n",
      "\n",
      "4. **Customization Capabilities**: Allows users to privately fine-tune models with their own data\n",
      "\n",
      "5. **AWS Integration**: Seamlessly works with existing AWS tools including:\n",
      "   - Amazon SageMaker ML features\n",
      "   - Experiments functionality for model testing\n",
      "   - Pipelines for managing FMs at scale\n",
      "\n",
      "## Implications\n",
      "\n",
      "1. **Democratization of AI**: Makes advanced AI capabilities accessible to a broader range of developers and companies\n",
      "\n",
      "2. **Reduced Technical Barriers**: The serverless approach removes infrastructure management complexity\n",
      "\n",
      "3. **Competitive Positioning**: Puts AWS in direct competition with other AI platform providers\n",
      "\n",
      "4. **Ecosystem Expansion**: Strengthens AWS's position by incorporating both proprietary and third-party AI models\n",
      "\n",
      "5. **Enterprise Adoption Acceleration**: The familiar AWS integration points likely speed up enterprise implementation of generative AI solutions\n",
      "\n",
      "The service represents AWS's strategic response to the growing demand for accessible, scalable, and secure generative AI capabilities in the cloud.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 273\n",
      ":Output tokens: 381\n",
      ":Total tokens: 654\n",
      "Latency: 8292 milliseconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Only run this when you're ready to see streaming output\n",
    "streamed_response = stream_converse(\n",
    "    MODELS[\"Claude 3.7 Sonnet\"], \n",
    "    streaming_request, \n",
    "    inference_config={\"temperature\": 0.4, \"maxTokens\": 1000}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c994e7e-f48d-454a-ab32-5c45a9f88482",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_generation_prompt = \"\"\"\n",
    "Create a Python function called get_weather that accepts a location as parameter. \\\n",
    "The function should return a dictionary containing weather data (condition, temperature, and humidity) for predefined cities.\\\n",
    "Use a mock data structure instead of actual API calls. Include New York, San Francisco, Miami, and Seattle as default cities.\\\n",
    "The return statement should look like the following: return weather_data.get(location, {\"condition\": \"Unknown\", \"temperature\": 0, \"humidity\": 0}).\n",
    "Only return the function and no preamble or examples.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5052af04-ce5b-406a-af72-2acfdc9aa50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "converse_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"{code_generation_prompt}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"temperature\": 0.0,\n",
    "        \"topP\": 0.9,\n",
    "        \"maxTokens\": 500\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6a59a42-e658-4e7a-9d10-7d1728217bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Converse API)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def get_weather(location):\n",
       "    weather_data = {\n",
       "        \"New York\": {\"condition\": \"Cloudy\", \"temperature\": 72, \"humidity\": 65},\n",
       "        \"San Francisco\": {\"condition\": \"Foggy\", \"temperature\": 62, \"humidity\": 80},\n",
       "        \"Miami\": {\"condition\": \"Sunny\", \"temperature\": 85, \"humidity\": 75},\n",
       "        \"Seattle\": {\"condition\": \"Rainy\", \"temperature\": 58, \"humidity\": 90}\n",
       "    }\n",
       "    return weather_data.get(location, {\"condition\": \"Unknown\", \"temperature\": 0, \"humidity\": 0})\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "        messages=converse_request[\"messages\"],\n",
    "        inferenceConfig=converse_request[\"inferenceConfig\"]\n",
    "    )\n",
    "    \n",
    "    # Extract the model's response\n",
    "    claude_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    display_response(claude_converse_response, \"Claude 3.7 Sonnet (Converse API)\")\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
    "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c917d31-80a9-4a3f-b786-46c27ac8dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location):\n",
    "    weather_data = {\n",
    "        \"New York\": {\"condition\": \"Cloudy\", \"temperature\": 72, \"humidity\": 65},\n",
    "        \"San Francisco\": {\"condition\": \"Foggy\", \"temperature\": 62, \"humidity\": 80},\n",
    "        \"Miami\": {\"condition\": \"Sunny\", \"temperature\": 85, \"humidity\": 75},\n",
    "        \"Seattle\": {\"condition\": \"Rainy\", \"temperature\": 58, \"humidity\": 90}\n",
    "    }\n",
    "    return weather_data.get(location, {\"condition\": \"Unknown\", \"temperature\": 0, \"humidity\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78b45977-0b46-4d18-9d87-9361b39de31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_tool = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get current weather for a specific location\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city name to get weather for\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"location\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"toolChoice\": {\n",
    "        \"auto\": {}  # Let the model decide when to use the tool\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2198798-424e-4fe1-b643-c5970e42c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": \"What's the weather like in San Francisco right now? And what should I wear?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"temperature\": 0.0,  # Use 0 temperature for deterministic function calling\n",
    "        \"maxTokens\": 500\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "149e8d26-bde4-418f-a331-4e3e2a1faae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"69ff54fb-7848-443d-a4f3-c6ee99468ada\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Tue, 27 May 2025 08:02:03 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"531\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"69ff54fb-7848-443d-a4f3-c6ee99468ada\"\n",
      "    },\n",
      "    \"RetryAttempts\": 1\n",
      "  },\n",
      "  \"output\": {\n",
      "    \"message\": {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": \"I can check the current weather in San Francisco for you and provide clothing recommendations based on that information.\"\n",
      "        },\n",
      "        {\n",
      "          \"toolUse\": {\n",
      "            \"toolUseId\": \"tooluse_TQ6SJEM0RkeCqgd8dVLS0A\",\n",
      "            \"name\": \"get_weather\",\n",
      "            \"input\": {\n",
      "              \"location\": \"San Francisco\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"stopReason\": \"tool_use\",\n",
      "  \"usage\": {\n",
      "    \"inputTokens\": 404,\n",
      "    \"outputTokens\": 75,\n",
      "    \"totalTokens\": 479\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"latencyMs\": 3328\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = bedrock.converse(\n",
    "    modelId=MODELS[\"Claude 3.7 Sonnet\"],\n",
    "    messages=function_request[\"messages\"],\n",
    "    inferenceConfig=function_request[\"inferenceConfig\"],\n",
    "    toolConfig=weather_tool\n",
    ")\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aa6a545-83e0-4cd3-b8fe-073949148c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_function_calling(model_id, request, tool_config):\n",
    "    try:\n",
    "        # Step 1: Send initial request\n",
    "        response = bedrock.converse(\n",
    "            modelId=model_id,\n",
    "            messages=request[\"messages\"],\n",
    "            inferenceConfig=request[\"inferenceConfig\"],\n",
    "            toolConfig=tool_config\n",
    "        )\n",
    "        \n",
    "        # Check if the model wants to use a tool (check the correct response structure)\n",
    "        content_blocks = response[\"output\"][\"message\"][\"content\"]\n",
    "        has_tool_use = any(\"toolUse\" in block for block in content_blocks)\n",
    "        \n",
    "        if has_tool_use:\n",
    "            # Find the toolUse block\n",
    "            tool_use_block = next(block for block in content_blocks if \"toolUse\" in block)\n",
    "            tool_use = tool_use_block[\"toolUse\"]\n",
    "            tool_name = tool_use[\"name\"]\n",
    "            tool_input = tool_use[\"input\"]\n",
    "            tool_use_id = tool_use[\"toolUseId\"]\n",
    "            \n",
    "            # Step 2: Execute the tool\n",
    "            if tool_name == \"get_weather\":\n",
    "                tool_result = get_weather(tool_input[\"location\"])\n",
    "            else:\n",
    "                tool_result = {\"error\": f\"Unknown tool: {tool_name}\"}\n",
    "            \n",
    "            # Step 3: Send the tool result back to the model\n",
    "            updated_messages = request[\"messages\"] + [\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"toolUse\": {\n",
    "                                \"toolUseId\": tool_use_id,\n",
    "                                \"name\": tool_name,\n",
    "                                \"input\": tool_input\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"toolResult\": {\n",
    "                                \"toolUseId\": tool_use_id,\n",
    "                                \"content\": [\n",
    "                                    {\n",
    "                                        \"json\": tool_result\n",
    "                                    }\n",
    "                                ],\n",
    "                                \"status\": \"success\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Step 4: Get final response\n",
    "            final_response = bedrock.converse(\n",
    "                modelId=model_id,\n",
    "                messages=updated_messages,\n",
    "                inferenceConfig=request[\"inferenceConfig\"],\n",
    "                toolConfig=tool_config  \n",
    "            )\n",
    "            \n",
    "            # Extract text from the correct response structure\n",
    "            final_text = \"\"\n",
    "            for block in final_response[\"output\"][\"message\"][\"content\"]:\n",
    "                if \"text\" in block:\n",
    "                    final_text = block[\"text\"]\n",
    "                    break\n",
    "            \n",
    "            return {\n",
    "                \"tool_call\": {\"name\": tool_name, \"input\": tool_input},\n",
    "                \"tool_result\": tool_result,\n",
    "                \"final_response\": final_text\n",
    "            }\n",
    "        else:\n",
    "            # Model didn't use a tool, just return the text response\n",
    "            text_response = \"\"\n",
    "            for block in content_blocks:\n",
    "                if \"text\" in block:\n",
    "                    text_response = block[\"text\"]\n",
    "                    break\n",
    "                    \n",
    "            return {\n",
    "                \"final_response\": text_response\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in function calling: {str(e)}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0771e99-dc02-4371-8ef5-090f687f67c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call: get_weather({'location': 'San Francisco'})\n",
      "Tool Result: {'condition': 'Foggy', 'temperature': 62, 'humidity': 80}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Function Calling)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Currently in San Francisco, it's 62°F with foggy conditions and 80% humidity.\n",
       "\n",
       "For this weather, I'd recommend:\n",
       "- A light jacket or sweater as your outer layer\n",
       "- Long pants or jeans\n",
       "- A t-shirt or light long-sleeve shirt underneath\n",
       "- Comfortable shoes\n",
       "\n",
       "The foggy conditions mean it might feel a bit cooler than the temperature suggests, especially if you'll be near the water. Layers are always a good idea in San Francisco as the weather can change throughout the day. You might want to bring a light scarf or hat if you're sensitive to the damp, cool air that comes with fog."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function_result = handle_function_calling(\n",
    "    MODELS[\"Claude 3.7 Sonnet\"], \n",
    "    function_request,\n",
    "    weather_tool\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "if \"error\" not in function_result:\n",
    "    if \"tool_call\" in function_result:\n",
    "        print(f\"Tool Call: {function_result['tool_call']['name']}({function_result['tool_call']['input']})\")\n",
    "        print(f\"Tool Result: {function_result['tool_result']}\")\n",
    "    \n",
    "    display_response(function_result[\"final_response\"], \"Claude 3.7 Sonnet (Function Calling)\")\n",
    "else:\n",
    "    print(f\"Error: {function_result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c430bd5-8ba9-457b-aa78-0d37358285f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
